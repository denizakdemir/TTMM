{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTML Classification Examples\n",
    "\n",
    "This notebook demonstrates using the TTML model for various classification tasks. We'll cover:\n",
    "\n",
    "1. Adult Income Classification\n",
    "   - Binary classification predicting income >50K\n",
    "   - Handling mixed categorical and numerical features\n",
    "   - Feature importance analysis\n",
    "\n",
    "2. Titanic Survival Classification\n",
    "   - Advanced preprocessing techniques\n",
    "   - Model interpretation\n",
    "   - Performance visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Import TTML modules\n",
    "from tabular_transformer.models import TabularTransformer\n",
    "from tabular_transformer.models.task_heads import ClassificationHead\n",
    "from tabular_transformer.training import Trainer\n",
    "from tabular_transformer.inference import predict\n",
    "from tabular_transformer.explainability import global_explanations, local_explanations\n",
    "from tabular_transformer.utils.config import TransformerConfig\n",
    "from tabular_transformer.data.dataset import TabularDataset\n",
    "\n",
    "# Import data utilities\n",
    "from data_utils import download_adult_dataset, download_titanic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Adult Income Classification\n",
    "\n",
    "First, we'll work with the Adult Income dataset to predict whether an individual's income exceeds $50K/year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult dataset shape: (32561, 15)\n",
      "\n",
      "Feature types:\n",
      "age               int64\n",
      "workclass        object\n",
      "fnlwgt            int64\n",
      "education        object\n",
      "education-num     int64\n",
      "marital-status   object\n",
      "occupation       object\n",
      "relationship     object\n",
      "race             object\n",
      "sex              object\n",
      "capital-gain      int64\n",
      "capital-loss      int64\n",
      "hours-per-week    int64\n",
      "native-country   object\n",
      "class            object\n",
      "dtype: object\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "<=50K    0.759281\n",
      ">50K     0.240719\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Download Adult dataset\n",
    "adult_df = download_adult_dataset(save_csv=False)\n",
    "print(\"Adult dataset shape:\", adult_df.shape)\n",
    "print(\"\\nFeature types:\")\n",
    "print(adult_df.dtypes)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(adult_df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-17 13:17:54,694 - tabular_transformer.FeaturePreprocessor - INFO - Fitted numeric scaler to 6 columns\n",
      "2025-03-17 13:17:54,701 - tabular_transformer.FeaturePreprocessor - INFO - Column workclass: 9 categories, embedding dim 4\n",
      "2025-03-17 13:17:54,703 - tabular_transformer.FeaturePreprocessor - INFO - Column education: 16 categories, embedding dim 8\n",
      "2025-03-17 13:17:54,705 - tabular_transformer.FeaturePreprocessor - INFO - Column marital-status: 7 categories, embedding dim 4\n",
      "2025-03-17 13:17:54,707 - tabular_transformer.FeaturePreprocessor - INFO - Column occupation: 15 categories, embedding dim 7\n",
      "2025-03-17 13:17:54,709 - tabular_transformer.FeaturePreprocessor - INFO - Column relationship: 6 categories, embedding dim 3\n",
      "2025-03-17 13:17:54,711 - tabular_transformer.FeaturePreprocessor - INFO - Column race: 5 categories, embedding dim 3\n",
      "2025-03-17 13:17:54,713 - tabular_transformer.FeaturePreprocessor - INFO - Column sex: 2 categories, embedding dim 2\n",
      "2025-03-17 13:17:54,715 - tabular_transformer.FeaturePreprocessor - INFO - Column native-country: 42 categories, embedding dim 16\n",
      "2025-03-17 13:17:54,774 - tabular_transformer.TabularDataset - INFO - Created dataset with 26048 samples, 6 numeric features, 8 categorical features, 1 tasks\n",
      "2025-03-17 13:17:54,803 - tabular_transformer.TabularDataset - INFO - Created dataset with 6513 samples, 6 numeric features, 8 categorical features, 1 tasks\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = adult_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = adult_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target column from features\n",
    "target_column = 'class'\n",
    "if target_column in numeric_features:\n",
    "    numeric_features.remove(target_column)\n",
    "if target_column in categorical_features:\n",
    "    categorical_features.remove(target_column)\n",
    "\n",
    "# Create train/test datasets\n",
    "# Preprocess the class column to convert string labels to numeric\n",
    "print(\"Original class values:\", adult_df['class'].unique())\n",
    "adult_df['class'] = adult_df['class'].map({'>50K': 1, '<=50K': 0})\n",
    "print(\"Converted class values:\", adult_df['class'].unique())\n",
    "\n",
    "# Preprocess the class column to convert string labels to numeric\n",
    "print(\"Original class values:\", adult_df['class'].unique())\n",
    "adult_df['class'] = adult_df['class'].map({'>50K': 1, '<=50K': 0})\n",
    "print(\"Converted class values:\", adult_df['class'].unique())\n",
    "\n",
    "train_dataset_adult, test_dataset_adult, _ = TabularDataset.from_dataframe(\n",
    "    dataframe=adult_df,\n",
    "    numeric_columns=numeric_features,\n",
    "    categorical_columns=categorical_features,\n",
    "    target_columns={'main': [target_column]},\n",
    "    validation_split=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimensions from preprocessor\n",
    "feature_dims = train_dataset_adult.preprocessor.get_feature_dimensions()\n",
    "numeric_dim = feature_dims['numeric_dim']\n",
    "categorical_dims = feature_dims['categorical_dims']\n",
    "categorical_embedding_dims = feature_dims['categorical_embedding_dims']\n",
    "\n",
    "# Model configuration\n",
    "config = TransformerConfig(\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    dropout=0.2,\n",
    "    variational=False\n",
    ")\n",
    "\n",
    "# Initialize transformer encoder\n",
    "encoder_adult = TabularTransformer(\n",
    "    numeric_dim=numeric_dim,\n",
    "    categorical_dims=categorical_dims,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize classification head\n",
    "task_head_adult = ClassificationHead(\n",
    "    name=\"main\",  # Task name should match the key in target_columns\n",
    "    input_dim=128,  # Should match config.embed_dim\n",
    "    num_classes=2  # Binary classification for income\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-17 13:18:27,694 - tabular_transformer.Trainer - INFO - Starting training for 20 epochs\n",
      "2025-03-17 13:18:27,696 - tabular_transformer.Trainer - INFO - Using device: cpu\n",
      "2025-03-17 13:18:27,697 - tabular_transformer.Trainer - INFO - Epoch 1/20\n",
      "2025-03-17 13:19:14,654 - tabular_transformer.Trainer - INFO - Training loss: 0.6821, Validation loss: 0.6801\n",
      "2025-03-17 13:19:14,656 - tabular_transformer.Trainer - INFO - Epoch 2/20\n",
      "2025-03-17 13:20:01,544 - tabular_transformer.Trainer - INFO - Training loss: 0.6611, Validation loss: 0.6593\n",
      "2025-03-17 13:20:01,546 - tabular_transformer.Trainer - INFO - Epoch 3/20\n",
      "2025-03-17 13:20:48,345 - tabular_transformer.Trainer - INFO - Training loss: 0.6401, Validation loss: 0.6384\n",
      "2025-03-17 13:20:48,347 - tabular_transformer.Trainer - INFO - Epoch 4/20\n",
      "2025-03-17 13:21:35,246 - tabular_transformer.Trainer - INFO - Training loss: 0.6192, Validation loss: 0.6176\n",
      "2025-03-17 13:21:35,248 - tabular_transformer.Trainer - INFO - Epoch 5/20\n",
      "2025-03-17 13:22:22,145 - tabular_transformer.Trainer - INFO - Training loss: 0.5984, Validation loss: 0.5967\n",
      "2025-03-17 13:22:22,147 - tabular_transformer.Trainer - INFO - Epoch 6/20\n",
      "2025-03-17 13:23:09,046 - tabular_transformer.Trainer - INFO - Training loss: 0.5775, Validation loss: 0.5759\n",
      "2025-03-17 13:23:09,048 - tabular_transformer.Trainer - INFO - Epoch 7/20\n",
      "2025-03-17 13:23:55,947 - tabular_transformer.Trainer - INFO - Training loss: 0.5567, Validation loss: 0.5552\n",
      "2025-03-17 13:23:55,949 - tabular_transformer.Trainer - INFO - Epoch 8/20\n",
      "2025-03-17 13:24:42,848 - tabular_transformer.Trainer - INFO - Training loss: 0.5358, Validation loss: 0.5344\n",
      "2025-03-17 13:24:42,850 - tabular_transformer.Trainer - INFO - Epoch 9/20\n",
      "2025-03-17 13:25:29,749 - tabular_transformer.Trainer - INFO - Training loss: 0.5150, Validation loss: 0.5136\n",
      "2025-03-17 13:25:29,751 - tabular_transformer.Trainer - INFO - Epoch 10/20\n",
      "2025-03-17 13:26:16,650 - tabular_transformer.Trainer - INFO - Training loss: 0.4942, Validation loss: 0.4929\n",
      "2025-03-17 13:26:16,652 - tabular_transformer.Trainer - INFO - Epoch 11/20\n",
      "2025-03-17 13:27:03,551 - tabular_transformer.Trainer - INFO - Training loss: 0.4734, Validation loss: 0.4722\n",
      "2025-03-17 13:27:03,553 - tabular_transformer.Trainer - INFO - Early stopping triggered. Best epoch: 11\n",
      "2025-03-17 13:27:03,554 - tabular_transformer.Trainer - INFO - Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "train_loader_adult = train_dataset_adult.create_dataloader(batch_size=64, shuffle=True)\n",
    "test_loader_adult = test_dataset_adult.create_dataloader(batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_adult = Trainer(\n",
    "    encoder=encoder_adult,\n",
    "    task_head={'main': task_head_adult},  # Map task head to task name\n",
    "    optimizer=None,  # Will be created by trainer\n",
    "    device=None  # Will use CUDA if available\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_adult = trainer_adult.train(\n",
    "    train_loader=train_loader_adult,\n",
    "    val_loader=test_loader_adult,\n",
    "    num_epochs=20,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Income Classification Results:\n",
      "Accuracy: 0.8489\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      4945\n",
      "           1       0.71      0.65      0.68      1568\n",
      "\n",
      "    accuracy                           0.85      6513\n",
      "   macro avg       0.80      0.78      0.79      6513\n",
      "weighted avg       0.85      0.85      0.85      6513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions_adult = trainer_adult.predict(test_loader_adult)\n",
    "\n",
    "# Get predictions for the main task\n",
    "y_pred_adult = predictions_adult['main']['predicted_class'].numpy()\n",
    "y_test_adult = test_dataset_adult.targets['main']\n",
    "\n",
    "# Print metrics\n",
    "print(\"Adult Income Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_adult, y_pred_adult):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_adult, y_pred_adult))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for income prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "feature_importance = global_explanations.calculate_feature_importance(\n",
    "    encoder=encoder_adult,\n",
    "    task_head=task_head_adult,\n",
    "    dataset=test_dataset_adult,\n",
    "    feature_names=numeric_features + categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Titanic Survival Classification\n",
    "\n",
    "Now we'll demonstrate some advanced techniques with the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Titanic dataset\n",
    "titanic_df = download_titanic_dataset(save_csv=False)\n",
    "print(\"Titanic dataset shape:\", titanic_df.shape)\n",
    "print(\"\\nFeature types:\")\n",
    "print(titanic_df.dtypes)\n",
    "print(\"\\nSurvival distribution:\")\n",
    "print(titanic_df['survived'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = titanic_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = titanic_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target column from features\n",
    "target_column = 'survived'\n",
    "if target_column in numeric_features:\n",
    "    numeric_features.remove(target_column)\n",
    "if target_column in categorical_features:\n",
    "    categorical_features.remove(target_column)\n",
    "\n",
    "# Create train/test datasets\n",
    "train_dataset_titanic, test_dataset_titanic, _ = TabularDataset.from_dataframe(\n",
    "    dataframe=titanic_df,\n",
    "    numeric_columns=numeric_features,\n",
    "    categorical_columns=categorical_features,\n",
    "    target_columns={'main': [target_column]},\n",
    "    validation_split=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimensions from preprocessor\n",
    "feature_dims = train_dataset_titanic.preprocessor.get_feature_dimensions()\n",
    "numeric_dim = feature_dims['numeric_dim']\n",
    "categorical_dims = feature_dims['categorical_dims']\n",
    "categorical_embedding_dims = feature_dims['categorical_embedding_dims']\n",
    "\n",
    "# Model configuration\n",
    "config = TransformerConfig(\n",
    "    embed_dim=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    variational=False\n",
    ")\n",
    "\n",
    "# Initialize transformer encoder\n",
    "encoder_titanic = TabularTransformer(\n",
    "    numeric_dim=numeric_dim,\n",
    "    categorical_dims=categorical_dims,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize classification head\n",
    "task_head_titanic = ClassificationHead(\n",
    "    name=\"main\",  # Task name should match the key in target_columns\n",
    "    input_dim=64,  # Should match config.embed_dim\n",
    "    num_classes=2  # Binary classification for survival\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader_titanic = train_dataset_titanic.create_dataloader(batch_size=32, shuffle=True)\n",
    "test_loader_titanic = test_dataset_titanic.create_dataloader(batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_titanic = Trainer(\n",
    "    encoder=encoder_titanic,\n",
    "    task_head={'main': task_head_titanic},  # Map task head to task name\n",
    "    optimizer=None,  # Will be created by trainer\n",
    "    device=None  # Will use CUDA if available\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_titanic = trainer_titanic.train(\n",
    "    train_loader=train_loader_titanic,\n",
    "    val_loader=test_loader_titanic,\n",
    "    num_epochs=15,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_titanic = trainer_titanic.predict(test_loader_titanic)\n",
    "\n",
    "# Get predictions for the main task\n",
    "y_pred_titanic = predictions_titanic['main']['predicted_class'].numpy()\n",
    "y_test_titanic = test_dataset_titanic.targets['main']\n",
    "\n",
    "print(\"Titanic Survival Classification Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_titanic, y_pred_titanic):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_titanic, y_pred_titanic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Explanations\n",
    "\n",
    "Let's examine individual predictions to understand the model's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get local explanations for a few examples\n",
    "sample_indices = np.random.choice(len(test_dataset_titanic), 3, replace=False)\n",
    "for idx in sample_indices:\n",
    "    explanation = local_explanations.explain_prediction(\n",
    "        encoder=encoder_titanic,\n",
    "        task_head=task_head_titanic,\n",
    "        instance_idx=idx,\n",
    "        dataset=test_dataset_titanic,\n",
    "        feature_names=numeric_features + categorical_features\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExample {idx+1}:\")\n",
    "    print(f\"True class: {y_test_titanic[idx]}\")\n",
    "    print(f\"Predicted class: {y_pred_titanic[idx]}\")\n",
    "    print(\"\\nFeature contributions:\")\n",
    "    for feature, contribution in explanation.items():\n",
    "        print(f\"{feature}: {contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated advanced classification techniques using the TTML model on two different datasets:\n",
    "\n",
    "1. Adult Income Classification\n",
    "   - Achieved good performance on income prediction\n",
    "   - Identified key features through global importance analysis\n",
    "\n",
    "2. Titanic Survival Classification\n",
    "   - Demonstrated strong predictive performance\n",
    "   - Provided local explanations for individual predictions\n",
    "\n",
    "The TTML model showed its versatility in handling different types of classification tasks and its ability to provide interpretable results through various explanation techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}