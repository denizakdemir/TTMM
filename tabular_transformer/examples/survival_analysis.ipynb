{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTML Survival Analysis Examples\n",
    "\n",
    "This notebook demonstrates using the TTML model for survival analysis tasks using the SUPPORT dataset. We'll cover:\n",
    "\n",
    "1. Time-to-Event Prediction\n",
    "   - Survival time prediction\n",
    "   - Survival curve estimation\n",
    "   - Risk score calculation\n",
    "\n",
    "2. Competing Risks Analysis\n",
    "   - Multiple event types\n",
    "   - Cause-specific hazards\n",
    "   - Cumulative incidence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "# Import TTML modules\n",
    "from tabular_transformer.models import TabularTransformer\n",
    "from tabular_transformer.models.task_heads import SurvivalHead, CompetingRisksHead\n",
    "from tabular_transformer.training import Trainer\n",
    "from tabular_transformer.inference import predict\n",
    "from tabular_transformer.explainability import global_explanations, local_explanations\n",
    "from tabular_transformer.utils.config import TransformerConfig\n",
    "from tabular_transformer.data.dataset import TabularDataset\n",
    "\n",
    "# Import data utilities\n",
    "from data_utils import download_support_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Time-to-Event Prediction\n",
    "\n",
    "First, we'll work with the SUPPORT dataset to predict survival times and estimate survival curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SUPPORT dataset\n",
    "support_df = download_support_dataset(save_csv=False)\n",
    "print(\"SUPPORT dataset shape:\", support_df.shape)\n",
    "print(\"\\nFeature types:\")\n",
    "print(support_df.dtypes)\n",
    "print(\"\\nEvent distribution:\")\n",
    "print(support_df['death'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = support_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = support_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove time and event columns from features\n",
    "time_column = 'time'\n",
    "event_column = 'death'\n",
    "\n",
    "if time_column in numeric_features:\n",
    "    numeric_features.remove(time_column)\n",
    "if event_column in numeric_features:\n",
    "    numeric_features.remove(event_column)\n",
    "if time_column in categorical_features:\n",
    "    categorical_features.remove(time_column)\n",
    "if event_column in categorical_features:\n",
    "    categorical_features.remove(event_column)\n",
    "\n",
    "# Create train/test datasets\n",
    "train_dataset, test_dataset, _ = TabularDataset.from_dataframe(\n",
    "    dataframe=support_df,\n",
    "    numeric_columns=numeric_features,\n",
    "    categorical_columns=categorical_features,\n",
    "    target_columns={\n",
    "        'time': [time_column],\n",
    "        'event': [event_column]\n",
    "    },\n",
    "    validation_split=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature dimensions from preprocessor\n",
    "feature_dims = train_dataset.preprocessor.get_feature_dimensions()\n",
    "numeric_dim = feature_dims['numeric_dim']\n",
    "categorical_dims = feature_dims['categorical_dims']\n",
    "categorical_embedding_dims = feature_dims['categorical_embedding_dims']\n",
    "\n",
    "# Model configuration\n",
    "config = TransformerConfig(\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    dropout=0.2,\n",
    "    variational=False\n",
    ")\n",
    "\n",
    "# Initialize transformer encoder\n",
    "encoder = TabularTransformer(\n",
    "    numeric_dim=numeric_dim,\n",
    "    categorical_dims=categorical_dims,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize survival head\n",
    "survival_head = SurvivalHead(\n",
    "    input_dim=128,  # Should match config.embed_dim\n",
    "    num_time_bins=50  # Number of time intervals for discrete-time survival\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = train_dataset.create_dataloader(batch_size=64, shuffle=True)\n",
    "test_loader = test_dataset.create_dataloader(batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    encoder=encoder,\n",
    "    task_head=survival_head,\n",
    "    optimizer=None,  # Will be created by trainer\n",
    "    device=None  # Will use CUDA if available\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    num_epochs=25,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = trainer.predict(test_loader)\n",
    "\n",
    "# Get survival predictions and times/events\n",
    "survival_probs = predictions['main']['survival_probabilities']\n",
    "time_test = test_dataset.targets['time']\n",
    "event_test = test_dataset.targets['event']\n",
    "\n",
    "# Calculate concordance index\n",
    "c_index = survival_head.calculate_concordance_index(\n",
    "    survival_probs,\n",
    "    time_test,\n",
    "    event_test\n",
    ")\n",
    "\n",
    "print(f\"Concordance Index: {c_index:.4f}\")\n",
    "\n",
    "# Plot Kaplan-Meier curves for different risk groups\n",
    "risk_scores = survival_probs[:, -1].numpy()  # Use last time point for risk scoring\n",
    "risk_groups = pd.qcut(risk_scores, q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "for group in ['Low', 'Medium', 'High']:\n",
    "    mask = risk_groups == group\n",
    "    kmf.fit(\n",
    "        time_test[mask],\n",
    "        event_test[mask],\n",
    "        label=f'{group} Risk'\n",
    "    )\n",
    "    kmf.plot()\n",
    "\n",
    "plt.title('Survival Curves by Risk Group')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance for Survival\n",
    "\n",
    "Let's analyze which features are most important for survival prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot feature importance\n",
    "feature_importance = global_explanations.calculate_feature_importance(\n",
    "    encoder=encoder,\n",
    "    task_head=survival_head,\n",
    "    dataset=test_dataset,\n",
    "    feature_names=numeric_features + categorical_features\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_importance.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance for Survival Prediction')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Competing Risks Analysis\n",
    "\n",
    "Now we'll demonstrate competing risks analysis by considering different causes of death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test datasets with cause information\n",
    "train_dataset_cr, test_dataset_cr, _ = TabularDataset.from_dataframe(\n",
    "    dataframe=support_df,\n",
    "    numeric_columns=numeric_features,\n",
    "    categorical_columns=categorical_features,\n",
    "    target_columns={\n",
    "        'time': [time_column],\n",
    "        'event': [event_column],\n",
    "        'cause': ['cause']  # Additional cause column\n",
    "    },\n",
    "    validation_split=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize competing risks head\n",
    "competing_risks_head = CompetingRisksHead(\n",
    "    input_dim=128,  # Should match config.embed_dim\n",
    "    num_risks=3,  # Number of competing events\n",
    "    num_time_bins=50\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_cr = train_dataset_cr.create_dataloader(batch_size=64, shuffle=True)\n",
    "test_loader_cr = test_dataset_cr.create_dataloader(batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_cr = Trainer(\n",
    "    encoder=encoder,  # Reuse encoder from survival analysis\n",
    "    task_head=competing_risks_head,\n",
    "    optimizer=None,  # Will be created by trainer\n",
    "    device=None  # Will use CUDA if available\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_cr = trainer_cr.train(\n",
    "    train_loader=train_loader_cr,\n",
    "    val_loader=test_loader_cr,\n",
    "    num_epochs=25,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_cr = trainer_cr.predict(test_loader_cr)\n",
    "\n",
    "# Get competing risks predictions\n",
    "risk_probs = predictions_cr['main']['risk_probabilities']\n",
    "\n",
    "# Plot cumulative incidence functions\n",
    "plt.figure(figsize=(12, 6))\n",
    "time_points = np.linspace(0, max(test_dataset_cr.targets['time']), 100)\n",
    "\n",
    "for i in range(3):  # For each competing risk\n",
    "    cif = competing_risks_head.calculate_cumulative_incidence(\n",
    "        risk_probs[:, i, :],\n",
    "        time_points\n",
    "    )\n",
    "    plt.plot(time_points, cif.mean(axis=0), label=f'Cause {i+1}')\n",
    "\n",
    "plt.title('Cumulative Incidence Functions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cumulative Incidence')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Patient Analysis\n",
    "\n",
    "Let's examine predictions for individual patients to understand risk factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get local explanations for a few examples\n",
    "sample_indices = np.random.choice(len(test_dataset_cr), 3, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    explanation = local_explanations.explain_prediction(\n",
    "        encoder=encoder,\n",
    "        task_head=competing_risks_head,\n",
    "        instance_idx=idx,\n",
    "        dataset=test_dataset_cr,\n",
    "        feature_names=numeric_features + categorical_features\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPatient {idx+1}:\")\n",
    "    print(f\"Observed time: {test_dataset_cr.targets['time'][idx]:.1f}\")\n",
    "    print(f\"Event occurred: {bool(test_dataset_cr.targets['event'][idx])}\")\n",
    "    if test_dataset_cr.targets['event'][idx]:\n",
    "        print(f\"Cause: {test_dataset_cr.targets['cause'][idx]}\")\n",
    "    \n",
    "    print(\"\\nTop risk factors:\")\n",
    "    sorted_features = sorted(explanation.items(), key=lambda x: abs(x[1]), reverse=True)[:5]\n",
    "    for feature, contribution in sorted_features:\n",
    "        print(f\"{feature}: {contribution:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the survival analysis capabilities of the TTML model:\n",
    "\n",
    "1. Time-to-Event Prediction\n",
    "   - Successfully predicted survival times\n",
    "   - Generated interpretable survival curves\n",
    "   - Achieved good concordance index\n",
    "\n",
    "2. Competing Risks Analysis\n",
    "   - Modeled multiple causes of events\n",
    "   - Estimated cause-specific risks\n",
    "   - Provided individual risk assessments\n",
    "\n",
    "The TTML model showed strong performance in both standard survival analysis and competing risks scenarios, while providing valuable insights through its explainability features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
